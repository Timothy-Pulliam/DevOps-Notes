Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.4
Creation-Date: 2019-04-30T21:29:44-04:00

====== Regression Analysis ======
Created Tuesday 30 April 2019

== Linear Regression ==
{{./pasted_image.png}}

In linear regression, the observations (red) are assumed to be the result of random deviations (green) from an underlying relationship (blue) between a dependent variable (y) and an independent variable (x).

{{./equation.png?type=equation}}

Where {{./equation001.png?type=equation}} Denotes the transpose, so that {{./equation002.png?type=equation}} is the inner product between vectors {{./equation003.png?type=equation}} and  xi and Î².

Often these n equations are stacked together and written in matrix notation as

{{./equation004.png?type=equation}}

where

{{./equation005.png?type=equation}}
{{./equation006.png?type=equation}}
{{./equation007.png?type=equation}}


== Correlation Coefficient ==
{{.\pasted_image001.png}}
Several sets of (x, y) points, with the correlation coefficient of x and y for each set. Note that the correlation reflects the non-linearity and direction of a linear relationship (top row), but not the slope of that relationship (middle), nor many aspects of nonlinear relationships (bottom). N.B.: the figure in the center has a slope of 0 but in that case the correlation coefficient is undefined because the variance of Y is zero.
{{.\pasted_image002.png}}


== Example ==


'''
from statistics import mean
from math import sqrt

def correlationCoefficient(x,y):
    assert len(x) == len(y), "Datasets must be the same length"

    n = len(x)
    num1 = sum([i * j for i, j in zip(x,y)])
    num2 = n * mean(x) * mean(y)
    den1 = sum([i**2 for i in x]) - n * mean(x)**2
    den2 = sum([i**2 for i in y]) - n * mean(y)**2
    print((num1 - num2) / (sqrt(den1) * sqrt(den2)))
    
x = [2,5,8,10,17,40]
y = [100, 120, 150, 160, 230, 567]

correlationCoefficient(x,y)
'''

''0.9926412908333114''

== Determining the Equation of the Regression Line (Least Errors Squared Method) ==

'''
import numpy as np 
import matplotlib.pyplot as plt 

def estimate_coef(x, y): 
	# number of observations/points 
	n = np.size(x) 

	# mean of x and y vector 
	m_x, m_y = np.mean(x), np.mean(y) 

	# calculating cross-deviation and deviation about x 
	SS_xy = np.sum(y*x) - n*m_y*m_x 
	SS_xx = np.sum(x*x) - n*m_x*m_x 

	# calculating regression coefficients 
	b_1 = SS_xy / SS_xx 
	b_0 = m_y - b_1*m_x 

	return(b_0, b_1) 

def plot_regression_line(x, y, b): 
	# plotting the actual points as scatter plot 
	plt.scatter(x, y, color = "m", 
			marker = "o", s = 30) 

	# predicted response vector 
	y_pred = b[0] + b[1]*x 

	# plotting the regression line 
	plt.plot(x, y_pred, color = "g") 

	# putting labels 
	plt.xlabel('x') 
	plt.ylabel('y') 

	# function to show plot 
	plt.show() 

def main(): 
	# observations 
	x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 
	y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12]) 

	# estimating coefficients 
	b = estimate_coef(x, y) 
	print("Estimated coefficients:\nb_0 = {} \ 
		\nb_1 = {}".format(b[0], b[1])) 

	# plotting regression line 
	plot_regression_line(x, y, b) 

if __name__ == "__main__": 
	main()
'''
 




