Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.4
Creation-Date: 2018-07-02T16:41:58-04:00

====== S3 ======
Created Monday 02 July 2018
https://aws.amazon.com/s3/

==== Overview ====
https://en.wikipedia.org/wiki/Unstructured_data
https://www.youtube.com/watch?v=WV-TeM78mno
Object Storage. Object storage is useful for storing unstructured information, which is the majority of information stored. 

https://en.wikipedia.org/wiki/Object_storage
Object storage (also known as object-based storage) is a computer data storage architecture that manages data as objects, as opposed to other storage architectures like file systems which manage data as a file hierarchy, and block storage which manages data as blocks within sectors and tracks. Each object typically includes the data itself, a variable amount of metadata, and a globally unique identifier.

Object-storage systems allow retention of massive amounts of unstructured data. Object storage is used for purposes such as storing photos on Facebook, songs on Spotify, or files in online collaboration services, such as Dropbox.

S3 is **Object Storage** (think dropbox, google drive, etc.). It is used to store files, videos, images, etc. **It is not block storage**. You cannot install an operating system, or a database on it.

* Files can be 0-5TB
* There is unlimited Storage
* You pay by the GB
* Files are stored in "**Buckets**"
* Buckets = Universly Unique URI (https://s3-eu-west-1.amazonaws.com/bucketname)

**S3 has 11 9s of durability (99.999999999% durability)**

==== Data Consistency Model ====

== Read after write consistency for PUTS of new OBJECTS ==
We are able to read files immediately after uploading (PUTS) of new files (OBJECTS)

== Enventual Consistency for overwrite PUTS and Deletes (can take some time to propogate) ==
Changes to files (objects) can sometimes take time to propogate across multiple availability zones. As a result, if you update (PUT) a file (or delete a file) , you may still see the old version of the file. This is because of eventual consistency for overwrite PUTS and DELETES. Eventually your changes will propogate to all availability zones.


==== Objects ====
* Primary Key
* Information (images, text, video)
* Metadata
* Version ID
	* Subresources
	* ACLs
	* Torrents (for torrenting)

==== Features ====
* 99.9% Availability (uptime) Guarenteed (built for 99.99%)
* 11 9s (99.999999999%) Durability Guarenteed (File Integrity / File loss)
* Tiered Storage Available
* Lifecycle Management (if file is over Xdays old, archive to Glacier)
* Versioning of files
* Encryption
* Secure data with ACLs and Bucket Policies


===== Storage Classes =====
https://aws.amazon.com/s3/storage-classes/

== S3 - Standard ==
Amazon S3 Standard offers high durability, availability, and performance object storage for **frequently accessed data**. Because it delivers **low latency and high throughput**, S3 Standard is perfect for a wide variety of use cases including **cloud applications**, **dynamic websites**, **content distribution**, **mobile** and gaming applications, and Big Data analytics. S3 Lifecycle management offers configurable policies to automatically migrate objects to the most appropriate storage class.

Key Features:
* Low latency and high throughput performance
* Designed for durability of 99.999999999% (11 9s) of objects across multiple Availability Zones
* Data is resilient in the event of one entire Availability Zone destruction
* Designed for 99.99% availability over a given year
* Backed with the Amazon S3 Service Level Agreement for availability
* Supports SSL for data in transit and encryption of data at rest
* Lifecycle management for automatic migration of objects (objects can be archived to Glacier after set amount of time)
* Pricing
	https://aws.amazon.com/s3/pricing/
	* per GB
	* per request
	* Transfer Acceleration
	* Cross Region Replication
	* Storage Management Pricing

== S3 - IA (Infrequently Accessed) ==
Amazon S3 Standard-Infrequent Access (S3 Standard-IA) is an Amazon S3 storage class for data that is **accessed less frequently**, but requires **rapid access when needed**. S3 Standard-IA offers the high durability, high throughput, and low latency of S3 Standard, with a **low per GB storage price and per GB retrieval fee**. This combination of low cost and high performance make S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery. The S3 Standard-IA storage class is set at the object level and can exist in the same bucket as S3 Standard, allowing you to use S3 Lifecycle Policies to automatically transition objects between storage classes without any application changes.

Key Features:
* Same low latency and high throughput performance of S3 Standard
* Designed for durability of 99.999999999% (11 9s) of objects across multiple Availability Zones
* Data is resilient in the event of one entire Availability Zone destruction
* Designed for 99.9% availability over a given year
* Backed with the Amazon S3 Service Level Agreement for availability
* Supports SSL for data in transit and encryption of data at rest
* Lifecycle management for automatic migration of objects

* Data that is accessed less frequently
* Quick, rapid access (low latency once accessed)
* Lower fee than standard, but charged a retrieval fee
* Pricing
	* Data Retrievals	$0.01 per GB
	* Data Returned by S3 Select	$0.01 per GB
	* Data Scanned by S3 Select	$0.002 per GB
	* PUT, COPY, or POST Requests	$0.01 per 1,000 requests
	* GET, SELECT and all other Requests	$0.001 per 1,000 requests

== S3 - One Zone - IA ==
* Lower cost option for infrequently accessed data
* No resilience across multiple AZs 
* Pricing
	* Data Retrievals	$0.01 per GB
	* Data Returned by S3 Select	$0.01 per GB
	* Data Scanned by S3 Select	$0.002 per GB
	* PUT, COPY or POST Requests	$0.01 per 1,000 requests
	* GET, SELECT and all other Requests	$0.001 per 1,000 requests


== Glacier ==
Amazon Glacier is a **secure**, **durable**, **and extremely low-cost** storage service for data archiving. You can reliably store any amount of data at costs that are competitive with or cheaper than on-premises solutions. To keep costs low yet suitable for varying retrieval needs, Amazon Glacier provides three options for access to archives, from a few minutes to several hours. Amazon Glacier supports S3 Lifecycle Policies for automatic migration between S3 & Amazon Glacier storage classes. Please see the Amazon Glacier page for more details.

Key Features:

* Designed for durability of 99.999999999% of objects across multiple Availability Zones
* Data is resilient in the event of one entire Availability Zone destruction
* Supports SSL for data in transit and encryption of data at rest
* Vault Lock feature enforces compliance via a lockable WORM policy
* Extremely low cost design is ideal for long-term archive
* Lifecycle management for automatic migration of objects

* Very cheap
* Archival only (data stored for long periods of time)
https://docs.aws.amazon.com/amazonglacier/latest/dev/downloading-an-archive-two-steps.html#api-downloading-an-archive-two-steps-retrieval-options
* Pricing
	* Lifecycle Transition Requests into Glacier (placing data from S3 into Glacier)	$0.05 per 1,000 requests
	* Archive Retrieval Options: Expedited (1-5 Minutes, **$$$**), Standard (3-5 Hours, **$$**), Bulk (5-12 Hours, **$**)


{{.\pasted_image003.png}}

==== S3 Standard Charges ====
https://aws.amazon.com/s3/pricing/
* Storage (per GB)
* Requests
* Storage Management Pricing
* Data Transfers from one region to another (Cross Region Replication)
* Transfer Acceleration (Fast, easy, secure transfers between end users and an S3 bucket via [[CloudFront]] CDN). Data is routed through an optimized network path.

Below is an image displaying how Transfer Acceleration works.

* The end user (1) uploads files (objects) to a CDN edge location (2). The edge location then uploads to the S3 Bucket (3) via Amazons backbone network. This reduces the latency perceived by the end user. It also reduces the stress on the AWS S3 network backbone.

{{.\pasted_image002.png}}


==== Uploading Large Files Using Multipart API ====
https://docs.aws.amazon.com/AmazonS3/latest/dev/UploadingObjects.html
The total volume of data and number of objects you can store are unlimited. Individual Amazon S3 objects can range in size from a minimum of 0 bytes to a maximum of 5 terabytes. The largest object that can be uploaded in a single PUT is 5 gigabytes. For objects larger than 100 megabytes, customers should consider using the Multipart Upload capability.




==== FAQs ====
https://aws.amazon.com/s3/faqs/













































